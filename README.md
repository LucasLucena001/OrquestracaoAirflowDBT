# OrquestraÃ§Ã£o de Pipeline de Dados com Airflow + dbt

Este projeto simula um pipeline completo de dados para um e-commerce, utilizando **dbt** para modelagem e transformaÃ§Ã£o e **Apache Airflow (via Astro)** para orquestraÃ§Ã£o. 

A arquitetura foi pensada para suportar um processo moderno de engenharia de dados, com foco em reprodutibilidade, modularidade e escalabilidade.

---

## ğŸ”§ Stack utilizada

- Python 3.11
- dbt-core 1.10.5
- Apache Airflow via Astro CLI
- PostgreSQL 15
- Faker (para geraÃ§Ã£o de dados sintÃ©ticos)
- Docker (para isolar e subir o ambiente Airflow)

---

## ğŸ¯ Objetivo

Orquestrar com Airflow as etapas de:

1. GeraÃ§Ã£o de dados sintÃ©ticos (simulaÃ§Ã£o de um e-commerce)
2. Carga dos dados brutos (seeds)
3. TransformaÃ§Ãµes por camadas (raw > staging > intermediate > mart)
4. ExecuÃ§Ã£o automatizada dos testes dbt
5. ExposiÃ§Ã£o de tabelas finais (marts) para consumo por BI ou ML

---

## ğŸ“ Estrutura do projeto

```bash
â”œâ”€â”€ airflowdbt/
â”‚   â”œâ”€â”€ dags/                      # DAGs do Airflow
â”‚   â”œâ”€â”€ include/
â”‚   â”‚   â””â”€â”€ dbteco/                # Projeto dbt
â”‚   â”‚       â”œâ”€â”€ models/            # Modelos dbt (por camada)
â”‚   â”‚       â”œâ”€â”€ seeds/             # Dados gerados via script Python
â”‚   â”‚       â”œâ”€â”€ macros/            # Macros dbt
â”‚   â”‚       â”œâ”€â”€ snapshots/         # (nÃ£o usado)
â”‚   â”‚       â”œâ”€â”€ profiles.yml       # ConfiguraÃ§Ã£o do dbt para execuÃ§Ã£o no contÃªiner
â”‚   â”‚       â””â”€â”€ dbt_project.yml    # Metadata do projeto dbt
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
